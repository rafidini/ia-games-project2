"""
IA et Jeux - 03 Algo REINFORCE
Jan. 2022

Objectif :
  - Faire en sorte que la tige reste le plus vertical possible pendant 200 itÃ©rations. 

Etat :
  - Un vecteur de 4 valeurs indiquant : 
    - la position horizontale du chariot
    - l'angle de la tige par rapport Ã  la verticale
    - les dÃ©rivÃ©es de ces deux grandeurs

Actions : 
  - DÃ©placer le chariot sur la gauche (0) ou sur la droite (1)
  - Remarquez qu'il n'est pas possible de laisser le chariot immobile !

RÃ©compense : 
  - +1 Ã  chaque itÃ©ration

Arret :
  - Lorsque la tige fait par rapport Ã  la verticale un angle supÃ©rieur Ã  12Â°
  - Lorsque le chariot quitte la zone de jeu.
  - AprÃ¨s 200 itÃ©rations

"""
import gym
import time
from pyglet.window import key

# Environnement du jeu
env = gym.make('CartPole-v0')

def politique():
    """TODO
    RÃ©seau de neurones en forme standard pour dÃ©crire la politique (ðœ‹0) avec
    deux couches Linear de 20 neurones sur la couche intermÃ©diaire.

    L'entrÃ©e du rÃ©seau correspondra aux 4 valeurs associÃ©es Ã  l'Ã©tat du jeu
    et les sorties du rÃ©seau donnent 2 valeurs qui seront transformÃ©es en pro-
    babilitÃ©s en utilisant la fonction Softmax.
    Pour dÃ©terminer quelle action va Ãªtre sÃ©lectionnÃ©e suivant la politique (ðœ‹0),
    nous utiliserons la classe Categorical.

    Une fois la politique mise en place, on peut l'utiliser pour effectuer des si-
    mulations. Lancez le programme, Ã  ce niveau, le contrÃ´le va Ãªtre trÃ¨s mauvais
    car les poids des rÃ©seaux ont Ã©tÃ© initialisÃ©s au hasard. Si cette Ã©tape fonction-
    ne, la moitiÃ© du chemin a Ã©tÃ© effectuÃ©.
    """
    pass

def compute_loss():
    #
    """TODO
    Calcul la loss.
    """
    pass

def GetAction(x: int) -> int:
    """
    IA basique

    Parameters
    ----------
        x (int) Position du charriot
    
    Returns
    -------
        1 (droite) si charriot Ã  gauche sinon
        0 (gauche) si charriot Ã  droite.
    """
    return 1 if x < 0 else 0

# Parametres des simulations
N_SIMULATIONS: int = 10
LEARNING_RATE: float = 0.01
#optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)
results: dict = {
    'scores': []
}

# Main game loop
for n in range(N_SIMULATIONS):
    # Gestion d'une simulation
    done = False
    state = env.reset()
    TotalScore = 0
    env.reset()

    while not done :
        # Affiche la simulation
        w = env.render()

        # RÃ©cupÃ¨re la position du chariot
        x = state[0]

        # TODO Decide de l'action Ã  effectuer
        action = GetAction(x)
        # action, log_prob = politique(x)
        
        #Â Mise Ã  jour de l'environnement
        state, reward, done, _ = env.step(action)

        # TODO Sauvegarde reward & log proba
        TotalScore += reward

        # Frequence de rafraichissement de l'animation
        time.sleep(0.01)
    
    # TODO Calcul de la loss
    ...

    # TODO Descente du gradient
    #optimizer.zero_grad()
    #Loss.backward()
    #optimizer.step()

    # Log du score
    print(f"[{n+1}/{N_SIMULATIONS}] - Score final : {TotalScore}")
    results['scores'].append(TotalScore)

# Affichage des resultats
print()
print("-" * 30)
print(f'Nombre de simulations : {N_SIMULATIONS}')
print(f"Taux d'apprentissage  : {LEARNING_RATE}")
print()
print(f"Score moyen           : {sum(results['scores']) / len(results['scores'])}")
print("-" * 30)

env.close()

